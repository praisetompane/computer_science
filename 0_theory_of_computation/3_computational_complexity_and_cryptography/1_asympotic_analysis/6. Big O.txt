def = 
    language and metric   
        to describe efficiency of algorithms
    
An analogy

TIME COMPLEnITY
    Asymptotic runtime/Big O time
        - How does the runtime(runnig time) of the algorithm chnage,
              as the size of the problem changes

    Big O, Big Theta and Big Omega

    Three ways to describe an algorithm's runtime
        Best Case, Worst Case and Enpected Case

SPACE COMPLEnITY
    The amount of memory/space
        required by an algorithm

    e.g.
        array of n size = O(n) space complenty
        2D array of n*n = O(n^2) space complenty
        stack space used in recursive calls
            def fac(n):
                if n == 1: return 1
                else: return(n * fac(n - 1))
            
            For function above, O(n) space complexity on fac calls on the stack


DROP THE CONSTANTS
    Big O describes the rate of increase
          describes how the runtime scales

    Insight = e.g. Accept that O(N) isn't always better than O(N^2), for some inputs


DROP THE NON DETERMINANT CONSTANTS
    O(N^2 + N) should just be O(N)
    O(N^2 + N^2) should just be O(N^2)

COMMON BIG O RUNTIMES, BEST TO WORST

    O(1)     = constant time
    O(log n) = logarithmic time
    O(n) = linear time
    O(n log n) = 
    O(n^2) = 
    O(2^n) = enponential time
    O(n!) = 
    ...
    O(n^n)

MULTIPART ALGORITHMS
    algorithm with more than 1 step
        1. When do you add the steps?
        2. When do you multiply the steps?

        1. If an algorithm says do A chunk of work, then when you're all done, then do B chunk of work
            Book = do this, then, when you're all done, do that
            ref = 6.2.AddSteps.py

            O(A + B)

        2. If an algorithm says for every point in A do every point in Chunk B
            Book = do this for each time you do that
            ref = 6.2.MultiplySteps.py

            O(A * B)

AMORTISED TIME ???(COME BACK TO THIS)
    def = rumtime that descibes the worst case happens infrequently

    ArrayList
        insertion runtime:
            full => worst case = O(N + 1) => O(N)
                                rare
            common = O(1)

        Expl: during insertion, double capacity when size is power of 2
              double the capacity at size  1,2,4,8,16 ... X
                                     takes 1,2,4,8,16 ... X copies

                                     left to right = doubles until you get to X
                                     right to left = halves until you get to 1
                                     X + x/2 + x/4+ x/8 + ... 1 roughly 2X 

                                     X insertion takes O(2X) = armotised time for each insertion
                                     

log N RUNTIMES
    O(log N)
    Problem size is halved until its equal to 1

    16
    8   = 16/2
    4   = 8/2
    2   = 4/2
    1   = 2/2

    What is log?
        2^k = N
        What is the power to raise 2 to, to get N?
            i.e what is K?
            log N = K
                concrete examples:
                    log 2 = 1
                    log 16 = 3

RECURSIVE RUNTIMES
    int f (n) {
        if (n <= 1){
            return 1;
        }
        return f(n - 1) + f(n - 1)
    }

    my runtime
        O(N + N) => O(2N) => O(N)
        first f(n - 1) and f(n - 1) are loops
        INCORRECT! as subsequent calls in the first f(n - 1) each generate a pair of f(n - 1)
            that's not a loop through N elements(it's a lot more) 

    Common = O(branches ^ depth=N)
                branches = branch per recursive call

                Applies when there are more than 1 function calls per recursive node 

                Otherwise it's just O(N), much like a loop (asuuming ofcourse we go through each value of N)
    correct
        O(2^N)
        EXPLAIN??

EXCERCISES
    Why sorting is O(N log N)?
        Come back:
        Exercise 8
        Exercise 9
        Exercise 10
        Example 13


Progress = Page 3
            
